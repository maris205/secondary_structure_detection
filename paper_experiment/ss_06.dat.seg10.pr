Segmenting ss_06.dat.seg10 using 1-gram model
Boundary initialization: ran
Unigram generator: monkeys
 alpha0: 20, p_boundary: 0.5
Sampling of hyperparameters: OFF
Phoneme distribution: uniform
Sampling 1000 iterations
evaluating a sample
random seed = 1401848894
alphabet size = 22
Raising temperature in 10 increments: (0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1)

Reference lexicon summary:

Total words of each length (token/type): 
1: 754877 (31.32%) / 22 (0.00703678%)
2: 728827 (30.2391%) / 406 (0.129861%)
3: 308328 (12.7926%) / 7916 (2.53196%)
4: 291232 (12.0833%) / 63533 (20.3213%)
5: 132383 (5.49259%) / 64163 (20.5228%)
6: 62462 (2.59156%) / 54424 (17.4077%)
7: 42969 (1.78279%) / 40156 (12.844%)
8: 34047 (1.41262%) / 32007 (10.2376%)
9: 28071 (1.16467%) / 26020 (8.32259%)
10: 27014 (1.12082%) / 23996 (7.67521%)
Average word length: 2.66992 / 6.19672

Segmented lexicon summary:

Total words of each length (token/type): 
1: 1507874 (42.3%) / 22 (0.490305%)
2: 1340962 (37.6176%) / 384 (8.55806%)
3: 669449 (18.7799%) / 3454 (76.9779%)
4: 32695 (0.917184%) / 476 (10.6084%)
5: 1566 (0.0439306%) / 30 (0.668598%)
6: 2782 (0.0780427%) / 28 (0.624025%)
7: 2147 (0.0602292%) / 36 (0.802318%)
8: 3391 (0.0951268%) / 23 (0.512592%)
9: 1530 (0.0429207%) / 16 (0.356586%)
10: 984 (0.0276039%) / 7 (0.156006%)
11: 220 (0.0061716%) / 4 (0.0891464%)
12: 1112 (0.0311946%) / 3 (0.0668598%)
15: 1 (2.80527e-05%) / 1 (0.0222866%)
16: 1 (2.80527e-05%) / 1 (0.0222866%)
20: 1 (2.80527e-05%) / 1 (0.0222866%)
39: 1 (2.80527e-05%) / 1 (0.0222866%)
Average word length: 1.80521 / 3.16336

P 10.29 R 15.22 F 12.28 BP 37.58 BR 55.76 BF 44.9 LP 96.43 LR 1.384 LF 2.729
p_cont=0.993438, log prob = -2.1749e+07
