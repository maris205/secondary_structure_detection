Segmenting ss_06.dat.seg6 using 1-gram model
Boundary initialization: ran
Unigram generator: monkeys
 alpha0: 20, p_boundary: 0.5
Sampling of hyperparameters: OFF
Phoneme distribution: uniform
Sampling 1000 iterations
evaluating a sample
random seed = 1401791756
alphabet size = 22
Raising temperature in 10 increments: (0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1)

Reference lexicon summary:

Total words of each length (token/type): 
1: 855928 (31.7044%) / 22 (0.0115507%)
2: 876221 (32.456%) / 406 (0.213164%)
3: 354875 (13.1449%) / 7916 (4.15617%)
4: 375774 (13.919%) / 63533 (33.357%)
5: 162537 (6.02052%) / 64163 (33.6877%)
6: 74382 (2.75518%) / 54424 (28.5744%)
Average word length: 2.38361 / 4.86219

Segmented lexicon summary:

Total words of each length (token/type): 
1: 1595197 (44.1959%) / 22 (0.488889%)
2: 1300499 (36.0311%) / 384 (8.53333%)
3: 667312 (18.4883%) / 3461 (76.9111%)
4: 32957 (0.913094%) / 486 (10.8%)
5: 1635 (0.0452987%) / 39 (0.866667%)
6: 2086 (0.0577939%) / 26 (0.577778%)
7: 2090 (0.0579047%) / 27 (0.6%)
8: 3227 (0.089406%) / 23 (0.511111%)
9: 1928 (0.0534164%) / 15 (0.333333%)
10: 503 (0.0139359%) / 6 (0.133333%)
11: 1781 (0.0493437%) / 4 (0.0888889%)
12: 157 (0.00434978%) / 2 (0.0444444%)
14: 1 (2.77056e-05%) / 1 (0.0222222%)
18: 1 (2.77056e-05%) / 1 (0.0222222%)
23: 1 (2.77056e-05%) / 1 (0.0222222%)
25: 1 (2.77056e-05%) / 1 (0.0222222%)
35: 1 (2.77056e-05%) / 1 (0.0222222%)
Average word length: 1.78287 / 3.15978

P 12.04 R 16.09 F 13.77 BP 42.16 BR 56.48 BF 48.28 LP 95.96 LR 2.267 LF 4.43
p_cont=0.99352, log prob = -2.17606e+07
