Segmenting ss_06.dat.seg8 using 1-gram model
Boundary initialization: ran
Unigram generator: monkeys
 alpha0: 20, p_boundary: 0.5
Sampling of hyperparameters: OFF
Phoneme distribution: uniform
Sampling 1000 iterations
evaluating a sample
random seed = 1401819996
alphabet size = 22
Raising temperature in 10 increments: (0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1)

Reference lexicon summary:

Total words of each length (token/type): 
1: 795849 (31.2607%) / 22 (0.0083769%)
2: 795423 (31.244%) / 406 (0.154592%)
3: 330308 (12.9744%) / 7916 (3.01416%)
4: 331435 (13.0187%) / 63533 (24.1913%)
5: 146319 (5.74736%) / 64163 (24.4312%)
6: 64180 (2.52097%) / 54424 (20.7229%)
7: 43614 (1.71314%) / 40156 (15.2901%)
8: 38717 (1.52079%) / 32007 (12.1872%)
Average word length: 2.52767 / 5.57148

Segmented lexicon summary:

Total words of each length (token/type): 
1: 1589917 (44.0787%) / 22 (0.490087%)
2: 1302460 (36.1093%) / 384 (8.55424%)
3: 668738 (18.54%) / 3460 (77.0773%)
4: 32086 (0.889549%) / 474 (10.5591%)
5: 1831 (0.0507625%) / 38 (0.846514%)
6: 2543 (0.0705019%) / 28 (0.623747%)
7: 2035 (0.0564182%) / 29 (0.646024%)
8: 3280 (0.0909344%) / 16 (0.356427%)
9: 984 (0.0272803%) / 21 (0.46781%)
10: 2451 (0.0679513%) / 11 (0.245043%)
11: 666 (0.0184641%) / 2 (0.0445534%)
13: 1 (2.77239e-05%) / 1 (0.0222767%)
15: 1 (2.77239e-05%) / 1 (0.0222767%)
20: 1 (2.77239e-05%) / 1 (0.0222767%)
35: 1 (2.77239e-05%) / 1 (0.0222767%)
Average word length: 1.78405 / 3.15415

P 11.26 R 15.96 F 13.2 BP 39.92 BR 56.72 BF 46.86 LP 96.3 LR 1.646 LF 3.237
p_cont=0.993515, log prob = -2.17849e+07
