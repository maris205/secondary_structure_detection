Segmenting ss_06.dat.seg9 using 1-gram model
Boundary initialization: ran
Unigram generator: monkeys
 alpha0: 20, p_boundary: 0.5
Sampling of hyperparameters: OFF
Phoneme distribution: uniform
Sampling 1000 iterations
evaluating a sample
random seed = 1401834256
alphabet size = 22
Raising temperature in 10 increments: (0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1)

Reference lexicon summary:

Total words of each length (token/type): 
1: 774870 (31.2646%) / 22 (0.00762177%)
2: 761194 (30.7128%) / 406 (0.140656%)
3: 319230 (12.8803%) / 7916 (2.74245%)
4: 311930 (12.5858%) / 63533 (22.0106%)
5: 139465 (5.62715%) / 64163 (22.2289%)
6: 63271 (2.55287%) / 54424 (18.8549%)
7: 43131 (1.74026%) / 40156 (13.9118%)
8: 34510 (1.39241%) / 32007 (11.0886%)
9: 30828 (1.24385%) / 26020 (9.01447%)
Average word length: 2.59643 / 5.88054

Segmented lexicon summary:

Total words of each length (token/type): 
1: 1532186 (42.8658%) / 22 (0.486726%)
2: 1320550 (36.9449%) / 386 (8.53982%)
3: 675158 (18.8888%) / 3487 (77.146%)
4: 32847 (0.918956%) / 479 (10.5973%)
5: 1750 (0.0489595%) / 37 (0.818584%)
6: 2737 (0.0765727%) / 26 (0.575221%)
7: 1975 (0.0552543%) / 29 (0.641593%)
8: 2902 (0.0811889%) / 21 (0.464602%)
9: 1037 (0.029012%) / 16 (0.353982%)
10: 2702 (0.0755935%) / 11 (0.243363%)
11: 118 (0.00330127%) / 2 (0.0442478%)
12: 416 (0.0116384%) / 1 (0.0221239%)
13: 1 (2.79769e-05%) / 1 (0.0221239%)
15: 1 (2.79769e-05%) / 1 (0.0221239%)
36: 1 (2.79769e-05%) / 1 (0.0221239%)
Average word length: 1.80033 / 3.14934

P 10.7 R 15.43 F 12.64 BP 38.7 BR 55.98 BF 45.76 LP 96.44 LR 1.51 LF 2.974
p_cont=0.993456, log prob = -2.17582e+07
